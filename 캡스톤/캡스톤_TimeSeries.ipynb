{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49038196-6bb0-442f-9f14-dabd618f212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# 1. 라이브러리 임포트 (Libraries)\n",
    "#################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "#################################################################\n",
    "# 2. 데이터 로드 및 전처리 (Data Loading & Preprocessing)\n",
    "#################################################################\n",
    "\n",
    "def sliding_windows(data, seq_length):\n",
    "    \"\"\"\n",
    "    슬라이딩 윈도우를 적용하여 시퀀스 데이터(X)와 타겟(y)을 생성합니다.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        _x = data[i:(i + seq_length)] # (seq_length, 1)\n",
    "        _y = data[i + seq_length]     # (1,)\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "def load_and_preprocess_data(file_path, target_column, seq_length, test_size_ratio=0.15, val_size_ratio=0.15, batch_size=32):\n",
    "    \"\"\"\n",
    "    데이터를 로드하고, 정규화, 분할, 슬라이딩 윈도우 적용, DataLoader 생성을 수행합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} 파일을 찾을 수 없습니다.\")\n",
    "        # 예시로 사용할 airline-passengers 데이터를 다운로드합니다.\n",
    "        print(\"예시로 airline-passengers.csv 데이터를 다운로드합니다.\")\n",
    "        !wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
    "        file_path = 'airline-passengers.csv'\n",
    "        target_column = 'Passengers'\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "    try:\n",
    "        training_set_raw = df[target_column].values.astype(float)\n",
    "        training_set_raw = training_set_raw.reshape(-1, 1)\n",
    "    except KeyError:\n",
    "        print(f\"Error: '{target_column}' 열을 찾을 수 없습니다. 열 이름을 확인해주세요.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # 데이터 분할 (Train / Validation / Test)\n",
    "    total_size = len(training_set_raw)\n",
    "    test_size = int(total_size * test_size_ratio)\n",
    "    val_size = int(total_size * val_size_ratio)\n",
    "    train_size = total_size - test_size - val_size\n",
    "\n",
    "    train_data_raw = training_set_raw[0:train_size]\n",
    "    val_data_raw = training_set_raw[train_size - seq_length : train_size + val_size] # train_data의 마지막 seq_length 포함\n",
    "    test_data_raw = training_set_raw[train_size + val_size - seq_length :] # val_data의 마지막 seq_length 포함\n",
    "\n",
    "    # 정규화 (Scaler)\n",
    "    sc = StandardScaler()\n",
    "    train_data = sc.fit_transform(train_data_raw)\n",
    "    val_data = sc.transform(val_data_raw)\n",
    "    test_data = sc.transform(test_data_raw)\n",
    "\n",
    "    # 슬라이딩 윈도우 적용\n",
    "    x_train, y_train = sliding_windows(train_data, seq_length)\n",
    "    x_val, y_val = sliding_windows(val_data, seq_length)\n",
    "    x_test, y_test = sliding_windows(test_data, seq_length)\n",
    "    \n",
    "    # print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    # print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "    # print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    # Tensor로 변환\n",
    "    trainX = torch.Tensor(np.array(x_train))\n",
    "    trainY = torch.Tensor(np.array(y_train))\n",
    "    valX = torch.Tensor(np.array(x_val))\n",
    "    valY = torch.Tensor(np.array(y_val))\n",
    "    testX = torch.Tensor(np.array(x_test))\n",
    "    testY = torch.Tensor(np.array(y_test))\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_dataset = TensorDataset(trainX, trainY)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = TensorDataset(valX, valY)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = TensorDataset(testX, testY)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Data Loaded: Train({len(x_train)}), Validation({len(x_val)}), Test({len(x_test)})\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, sc\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# 3. 모델 정의 (Model Definitions)\n",
    "#################################################################\n",
    "\n",
    "# --- 모델 1: Linear Regression ---\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # 입력 차원을 [batch, seq_len, 1] -> [batch, seq_len]로 가정\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, features=1]\n",
    "        # 모델이 [batch, seq_len]을 기대하므로 squeeze\n",
    "        x = x.squeeze(-1) \n",
    "        out = self.linear(x)\n",
    "        return out.unsqueeze(-1) # [batch_size, 1] -> [batch_size, 1, 1] (y와 차원 맞춤)\n",
    "\n",
    "# --- 모델 2: MLP (Multi-Layer Perceptron) ---\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, features=1]\n",
    "        x = x.squeeze(-1) # [batch_size, seq_length]\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.unsqueeze(-1) # [batch_size, 1, 1]\n",
    "\n",
    "# --- 모델 3: Transformer (제공된 코드 기반) ---\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        d_h = d_model // n_heads\n",
    "        self.scale = d_h**-0.5\n",
    "        self.n_heads, self.d_h = n_heads, d_h\n",
    "        self.W_Q = nn.Linear(d_model, d_h * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_h * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_h * n_heads)\n",
    "        self.to_out = nn.Linear(n_heads * d_h, d_model)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        bs = q.size(0)\n",
    "        q_s = self.W_Q(q).view(bs, -1, self.n_heads, self.d_h)\n",
    "        k_s = self.W_K(k).view(bs, -1, self.n_heads, self.d_h)\n",
    "        v_s = self.W_V(v).view(bs, -1, self.n_heads, self.d_h)\n",
    "        attn_scores = torch.einsum('bphd, bshd -> bphs', q_s, k_s) * self.scale\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        output = torch.einsum('bphs, bshd -> bphd', attn_weights, v_s)\n",
    "        output = output.contiguous().view(bs, -1, self.n_heads*self.d_h)\n",
    "        output = self.to_out(output)\n",
    "        return output\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.ffn = nn.Sequential(nn.Linear(self.input_dim, 2*self.input_dim),\n",
    "                        nn.GELU(),\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(2*self.input_dim, self.input_dim))\n",
    "        self.layernorm = nn.LayerNorm(self.input_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.attn = AttentionBlock(self.input_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.attn(x,x,x)\n",
    "        x = x + self.dropout(y)\n",
    "        x = self.layernorm(x)\n",
    "        y = self.ffn(x)\n",
    "        x = x + self.dropout(y)\n",
    "        y = self.layernorm(x)\n",
    "        return y\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    # 원본 코드의 Transformer 클래스\n",
    "    # input_dim은 seq_length를 의미합니다.\n",
    "    def __init__(self, input_dim, emb_dim, out_dim, num_encoder):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim # seq_length\n",
    "        self.emb_dim = emb_dim\n",
    "        # 원본 코드는 seq_len을 2로 나누어 처리 (reshape(-1, 2, input_dim//2))\n",
    "        # 이 로직을 따르기 위해 input_layer의 입력 차원을 emb_dim//2로 설정\n",
    "        # (원래 코드에서 input_dim//2 였습니다)\n",
    "        self.input_layer = nn.Linear(input_dim//2, emb_dim//2, bias=True)\n",
    "        self.encoder_layers = nn.ModuleList([])\n",
    "        for i in range(num_encoder):\n",
    "            self.encoder_layers.append(EncoderBlock(emb_dim//2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.output_layer = nn.Linear(emb_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, 1]\n",
    "        x = x.squeeze(-1) # [batch_size, seq_length]\n",
    "        \n",
    "        # 원본 코드의 정규화 로직 (모델 내부에서)\n",
    "        mean = x.mean(dim=1,keepdim=True)\n",
    "        std = x.std(dim=1,keepdim=True) + 1e-6 # 0으로 나누는 것을 방지\n",
    "        x = (x-mean)/std\n",
    "        \n",
    "        # 원본 코드의 Reshape 로직 (seq_length=4 -> (2, 2))\n",
    "        # seq_length가 짝수여야 함\n",
    "        try:\n",
    "            x = x.reshape(-1, 2, self.input_dim//2) \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error: TransformerModel의 Reshape에 실패했습니다. seq_length({self.input_dim})가 짝수인지 확인하세요.\")\n",
    "            print(f\"입력 x shape: {x.shape}\")\n",
    "            raise e\n",
    "            \n",
    "        x = self.input_layer(x) # [batch, 2, emb_dim//2]\n",
    "        for l in self.encoder_layers:\n",
    "            x = l(x)\n",
    "        x = self.flatten(x) # [batch, 2 * (emb_dim//2)] = [batch, emb_dim]\n",
    "        x = self.output_layer(x) # [batch, 1]\n",
    "        \n",
    "        # 정규화 역변환\n",
    "        x = x*std+mean\n",
    "        return x.unsqueeze(-1) # [batch, 1, 1]\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# 4. 학습 및 검증 루프 (Training & Validation Loop)\n",
    "#################################################################\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, model_save_path='best_model.pth'):\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # 학습 모드\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 모델 포워딩 (모델 입력은 [batch, seq_len, 1]을 가정)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # 검증 (Validation)\n",
    "        model.eval() # 평가 모드\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.5f}, Val Loss: {avg_val_loss:.5f}')\n",
    "            \n",
    "        # 최고 성능 모델 저장\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            # print(f'New best model saved! Val Loss: {best_val_loss:.5f}')\n",
    "\n",
    "    print(f\"Training finished. Best validation loss: {best_val_loss:.5f}\")\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# 5. 메인 실행 (Main Execution)\n",
    "#################################################################\n",
    "\n",
    "# --- Hyperparameters & Configuration ---\n",
    "FILE_PATH = 'airline-passengers.csv'  # <-- (1) 여기에 파일 경로를 입력하세요.\n",
    "TARGET_COLUMN_NAME = 'Passengers'     # <-- (2) 여기에 예측할 열 이름을 입력하세요.\n",
    "MODEL_NAME = 'TRANSFORMER'            # <-- (3) 'LINEAR', 'MLP', 'TRANSFORMER' 중 선택\n",
    "\n",
    "SEQ_LENGTH = 12       # 시퀀스 길이 (Transformer는 짝수 권장)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2000     # Epoch 횟수\n",
    "LEARNING_RATE = 0.0001\n",
    "TEST_RATIO = 0.15\n",
    "VAL_RATIO = 0.15\n",
    "BEST_MODEL_PATH = 'best_model.pth'\n",
    "\n",
    "# --- 모델별 하이퍼파라미터 ---\n",
    "MLP_HIDDEN_DIM = 64\n",
    "TRANSFORMER_EMBED_DIM = 32  # 원본 코드 16 (emb_dim)\n",
    "TRANSFORMER_ENCODER_LAYERS = 3 # 원본 코드 3 (num_layers)\n",
    "# -----------------------------------\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. 데이터 로드\n",
    "train_loader, val_loader, test_loader, scaler = load_and_preprocess_data(\n",
    "    FILE_PATH, TARGET_COLUMN_NAME, SEQ_LENGTH, TEST_RATIO, VAL_RATIO, BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 2. 모델 선택 및 초기화\n",
    "if MODEL_NAME == 'LINEAR':\n",
    "    model = LinearModel(input_dim=SEQ_LENGTH, output_dim=1)\n",
    "    print(\"Model: Linear Regression\")\n",
    "elif MODEL_NAME == 'MLP':\n",
    "    model = MLPModel(input_dim=SEQ_LENGTH, hidden_dim=MLP_HIDDEN_DIM, output_dim=1)\n",
    "    print(f\"Model: MLP (Hidden dim: {MLP_HIDDEN_DIM})\")\n",
    "elif MODEL_NAME == 'TRANSFORMER':\n",
    "    if SEQ_LENGTH % 2 != 0:\n",
    "        print(f\"Warning: Transformer 모델은 seq_length가 짝수일 때 가장 잘 작동합니다. 현재: {SEQ_LENGTH}\")\n",
    "    model = TransformerModel(input_dim=SEQ_LENGTH, \n",
    "                               emb_dim=TRANSFORMER_EMBED_DIM, \n",
    "                               out_dim=1, \n",
    "                               num_encoder=TRANSFORMER_ENCODER_LAYERS)\n",
    "    print(f\"Model: Transformer (Embed dim: {TRANSFORMER_EMBED_DIM}, Layers: {TRANSFORMER_ENCODER_LAYERS})\")\n",
    "else:\n",
    "    raise ValueError(\"MODEL_NAME을 'LINEAR', 'MLP', 'TRANSFORMER' 중에서 선택해주세요.\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# 3. 손실 함수 및 옵티마이저\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 4. 모델 학습\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, NUM_EPOCHS, device, BEST_MODEL_PATH)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# 6. 테스트 및 결과 시각화 (Testing & Visualization)\n",
    "#################################################################\n",
    "\n",
    "print(\"\\n--- Testing ---\")\n",
    "# 저장된 최고 성능 모델 로드\n",
    "if MODEL_NAME == 'LINEAR':\n",
    "    best_model = LinearModel(input_dim=SEQ_LENGTH, output_dim=1)\n",
    "elif MODEL_NAME == 'MLP':\n",
    "    best_model = MLPModel(input_dim=SEQ_LENGTH, hidden_dim=MLP_HIDDEN_DIM, output_dim=1)\n",
    "elif MODEL_NAME == 'TRANSFORMER':\n",
    "    best_model = TransformerModel(input_dim=SEQ_LENGTH, \n",
    "                                    emb_dim=TRANSFORMER_EMBED_DIM, \n",
    "                                    out_dim=1, \n",
    "                                    num_encoder=TRANSFORMER_ENCODER_LAYERS)\n",
    "best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f'Test Loss (MSE): {avg_test_loss:.5f}')\n",
    "\n",
    "# 예측 결과 및 실제 값 취합\n",
    "predictions_np = np.concatenate(all_predictions).reshape(-1, 1)\n",
    "targets_np = np.concatenate(all_targets).reshape(-1, 1)\n",
    "\n",
    "# 정규화 역변환 (Inverse Transform)\n",
    "predictions_scaled = scaler.inverse_transform(predictions_np)\n",
    "targets_scaled = scaler.inverse_transform(targets_np)\n",
    "\n",
    "# 테스트셋의 실제 Y값 (전체 데이터 기준 인덱스)\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "full_data = df[TARGET_COLUMN_NAME].values.astype(float)\n",
    "test_start_index = len(full_data) - len(targets_scaled)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(full_data, label='Actual Full Data (Original)', color='gray', alpha=0.7)\n",
    "plt.plot(range(test_start_index, test_start_index + len(targets_scaled)), targets_scaled, label='Actual Test Data (Scaled)', color='blue', marker='.')\n",
    "plt.plot(range(test_start_index, test_start_index + len(predictions_scaled)), predictions_scaled, label='Predicted Test Data', color='red', linestyle='--', marker='x')\n",
    "\n",
    "plt.title(f'Time Series Forecasting - Model: {MODEL_NAME}')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel(TARGET_COLUMN_NAME)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
